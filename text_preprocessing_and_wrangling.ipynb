{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-preprocessing-and-wrangling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/text-analytics-with-python/blob/3-processing-and-understanding-text/text_preprocessing_and_wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsDEe5jehUQx",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing and Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK8QcV53hVML",
        "colab_type": "text"
      },
      "source": [
        "Text wrangling (also called preprocessing or normalization) is a process that consists of a series of steps to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and intelligent systems powered by machine learning and deep learning. \n",
        "\n",
        "Common techniques for preprocessing include-\n",
        "* cleaning text, \n",
        "* tokenizing text,\n",
        "* removing special characters, \n",
        "* case conversion, \n",
        "* correcting spellings, \n",
        "* removing stopwords\n",
        "* and other unnecessary terms, stemming, and lemmatization.\n",
        "\n",
        "The key idea is to remove unnecessary content from one or more text documents in a corpus (or corpora) and get clean text documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJnaD-5civaR",
        "colab_type": "text"
      },
      "source": [
        "## Removing HTML Tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2im0tkhciw4W",
        "colab_type": "text"
      },
      "source": [
        "Often, unstructured text contains a lot of noise, especially if you use techniques\n",
        "like web scraping or screen scraping to retrieve data from web pages, blogs, and\n",
        "online repositories. HTML tags, JavaScript, and Iframe tags typically don’t add much\n",
        "value to understanding and analyzing text. Our main intent is to extract meaningful\n",
        "textual content from the data extracted from the web.\n",
        "\n",
        "Let’s look at a section of a web page showing the King James version of the Bible.\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/text-analytics-with-python/bible.PNG?raw=1' width='800'/>\n",
        "\n",
        "We will now leverage requests and retrieve the contents of this web page in Python.\n",
        "This is known as web scraping and the following code helps us achieve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_2A3ZMi9iN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "08c28e10-7440-42f4-ec9a-87d7059493fb"
      },
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.content\n",
        "content[1163:2200]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'content=\"Ebookmaker 0.4.0a5 by Marcello Perathoner &lt;webmaster@gutenberg.org&gt;\" name=\"generator\"/>\\r\\n</head>\\r\\n  <body><p id=\"id00000\">Project Gutenberg EBook The Bible, King James, Book 1: Genesis</p>\\r\\n\\r\\n<p id=\"id00001\">Copyright laws are changing all over the world. Be sure to check the\\r\\ncopyright laws for your country before downloading or redistributing\\r\\nthis or any other Project Gutenberg eBook.</p>\\r\\n\\r\\n<p id=\"id00002\">This header should be the first thing seen when viewing this Project\\r\\nGutenberg file.  Please do not remove it.  Do not change or edit the\\r\\nheader without written permission.</p>\\r\\n\\r\\n<p id=\"id00003\">Please read the \"legal small print,\" and other information about the\\r\\neBook and Project Gutenberg at the bottom of this file.  Included is\\r\\nimportant information about your specific rights and restrictions in\\r\\nhow the file may be used.  You can also find out about how to make a\\r\\ndonation to Project Gutenberg, and how to get involved.</p>\\r\\n\\r\\n<p id=\"id00004\" style=\"margin-top: 2em\">**Welcome To The World of F'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkyy2hVZlmTA",
        "colab_type": "text"
      },
      "source": [
        "We can clearly see from the preceding output that it is extremely difficult to decipher the actual textual content in the web page, due to all the unnecessary HTML tags. We need to remove those tags. \n",
        "\n",
        "The BeautifulSoup library provides us with some handy\n",
        "functions that help us remove these unnecessary tags with ease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OarkpRtlTUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or7kU1a0lxlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "  # remove iframe and script tag\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "\n",
        "  return stripped_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLq5eY-9nExT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e2e06585-6f7d-4c5c-d696-c108093a9397"
      },
      "source": [
        "clean_content = strip_html_tags(content)\n",
        "clean_content[1163:2045]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\\nThis eBook was produced by David Widger\\nwith the help of Derek Andrew's text from January 1992\\nand the work of Bryan Taylor in November 2002.\\nBook 01        Genesis\\n01:001:001 In the beginning God created the heaven and the earth.\\n01:001:002 And the earth was without form, and void; and darkness was\\n           upon the face of the deep. And the Spirit of God moved upon\\n           the face of the waters.\\n01:001:003 And God said, Let there be light: and there was light.\\n01:001:004 And God saw the light, that it was good: and God divided the\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0light from the darkness.\\n01:001:005 And God called the light Day, and the darkness he called\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Night. And the evening and the morning were the first day.\\n01:001:006 And God said, Let there be a firmament in the midst of the\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0waters,\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027Hf_S3n6n5",
        "colab_type": "text"
      },
      "source": [
        "You can compare this output with the raw web page content and see that we have\n",
        "successfully removed the unnecessary HTML tags. We now have a clean body of text\n",
        "that’s easier to interpret and understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Ocjr86oOWj",
        "colab_type": "text"
      },
      "source": [
        "## Text Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcnrPoKeoPGH",
        "colab_type": "text"
      },
      "source": [
        "The most popular tokenization techniques include sentence and word tokenization, which are used to break down a text document (or corpus) into sentences and each sentence into words. Thus, tokenization can be defined as the process of breaking down or splitting textual data into smaller and more meaningful components called tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfm-7Ps6pA43",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU2oiOaKpB7C",
        "colab_type": "text"
      },
      "source": [
        "Sentence tokenization is the process of splitting a text corpus into sentences that act as the first level of tokens the corpus is comprised of. This is also known as sentence segmentation, since we try to segment the text into meaningful sentences.\n",
        "\n",
        "There are various ways to perform sentence tokenization. Basic techniques include:-\n",
        "* looking for specific delimiters between sentences like a period (.) \n",
        "* or a newline character (\\n) \n",
        "* and sometimes even a semicolon (;). \n",
        "\n",
        "We will use the NLTK framework, which provides various interfaces for performing sentence tokenization. We primarily focus on the\n",
        "following sentence tokenizers:\n",
        "* sent_tokenize\n",
        "* Pretrained sentence tokenization models\n",
        "* PunktSentenceTokenizer\n",
        "* RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9fJtf9pnNIi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0417e3db-553b-4470-ca52-883aea19c65c"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjOZQjkJqJNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading text corpora\n",
        "alice = gutenberg.raw(fileids='carroll-alice.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA0x15iTqbCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e275683c-0fd2-4db7-9d93-b16a0d8d7bae"
      },
      "source": [
        "sample_text = '''US unveils world's most powerful supercomputer, beats China. \\\n",
        " The US has unveiled the world's most powerful supercomputer called 'Summit', \\\n",
        " beating the previous record-holder China's Sunway TaihuLight. With a peak performance \\\n",
        " of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \\\n",
        " which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \\\n",
        " which reportedly take up the size of two tennis courts.'''\n",
        "sample_text"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China.  The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight. With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sEwRqy1rMwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "927be2ff-d73b-4395-98c5-7c892b03ab18"
      },
      "source": [
        "# Total characters in Alice in Wonderland\n",
        "len(alice)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144395"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg4nesUSsD1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4d6af57-4675-439f-c94f-48d23d5eed63"
      },
      "source": [
        "# First 100 characters in the corpus\n",
        "alice[:100]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNFeUIHUsNBQ",
        "colab_type": "text"
      },
      "source": [
        "#### Default Sentence Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNXcGcc8sOSc",
        "colab_type": "text"
      },
      "source": [
        "The nltk.sent_tokenize(...) function is the default sentence tokenization function\n",
        "that NLTK recommends and it uses an instance of the PunktSentenceTokenizer class\n",
        "internally. However, this is not just a normal object or instance of that class. It has been\n",
        "pretrained on several language models and works really well on many popular languages\n",
        "besides English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_18s3mQOs0ax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3d66f324-6bd3-4153-9127-5e05266095be"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFSwQubisIlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "829c9755-4d71-48ba-8961-6f3ab5515e60"
      },
      "source": [
        "default_st = nltk.sent_tokenize\n",
        "alice_sentences = default_st(text=alice)\n",
        "sample_sentences = default_st(text=sample_text)\n",
        "print(f'Total sentences in sample_text: {str(len(sample_sentences))}')\n",
        "print(f'Sample text sentences :-\\n{str(np.array(sample_sentences))}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences in sample_text: 4\n",
            "Sample text sentences :-\n",
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEL4tILvswmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "a9460f0a-40c5-4f29-9612-ebc946552949"
      },
      "source": [
        "print(f'\\nTotal sentences in alice: {str(len(alice_sentences))}')\n",
        "print(f'First 5 sentences in alice:- \\n {str(np.array(alice_sentences[:5]))}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total sentences in alice: 1625\n",
            "First 5 sentences in alice:- \n",
            " [\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
            " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
            " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
            " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
            " 'Oh dear!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzLoVASlt_oR",
        "colab_type": "text"
      },
      "source": [
        "Now, as you can see, the tokenizer is quite intelligent. It doesn’t just use periods to delimit sentences, but also considers other punctuation and capitalization of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5TG_HmVuFqj",
        "colab_type": "text"
      },
      "source": [
        "#### Pretrained Sentence Tokenizer Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WO0Rza1uHH-",
        "colab_type": "text"
      },
      "source": [
        "Suppose we were dealing with German text. We can use sent_tokenize, which\n",
        "is already trained, or load a pretrained tokenization model on German text into a PunktSentenceTokenizer instance and perform the same operation. The following\n",
        "snippet shows this. We start by loading a German text corpus and inspecting it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNMwlvXq2ERd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "228c04e0-cc70-497f-b3bd-ec076b39bd2e"
      },
      "source": [
        "nltk.download('europarl_raw')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eB8CmTtlXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a56f8aa0-3b68-4c13-e98c-01c4bca8c352"
      },
      "source": [
        "from nltk.corpus import europarl_raw\n",
        "\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "german_text[:100]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBejCRTt3ZuW",
        "colab_type": "text"
      },
      "source": [
        "Next, we tokenize the text corpus into sentences using the default sent_\n",
        "tokenize(...) tokenizer and a pretrained German language tokenizer by loading it\n",
        "from the NLTK resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gURIW3m2Bj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default sentence tokenizer\n",
        "german_sentences_def = default_st(text=german_text, language='german')\n",
        "\n",
        "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "german_sentences = german_tokenizer.tokenize(german_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfJ4siJp4I7Z",
        "colab_type": "text"
      },
      "source": [
        "We can now verify the time of our German tokenizer and check if the results\n",
        "obtained by using the two tokenizers match!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkFouiKI3_sT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88d80d86-6ced-45a0-ca47-3bfb93b7980f"
      },
      "source": [
        "# verify the type of german_tokenizer, should be PunktSentenceTokenizer\n",
        "type(german_tokenizer)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.tokenize.punkt.PunktSentenceTokenizer"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEMPwJpr4QHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ed66e77-4131-4c90-bb2a-aacb722bd6bd"
      },
      "source": [
        "# check if results of both tokenizers match , should be True\n",
        "(german_sentences_def == german_sentences)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJM8yTVA4mSL",
        "colab_type": "text"
      },
      "source": [
        "Thus we see that indeed the german_tokenizer is an instance of\n",
        "PunktSentenceTokenizer, which specializes in dealing with the German language. We\n",
        "also checked if the sentences obtained from the default tokenizer are the same as the\n",
        "sentences obtained by this pretrained tokenizer. As expected, they are the same (true)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK3fdDJI4bVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "7550f737-d402-4966-8ff4-19a82379fef8"
      },
      "source": [
        "# print first 5 sentences of the corpus\n",
        "np.array(german_sentences[:5])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
              "       'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
              "       'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
              "       'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
              "       'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .'],\n",
              "      dtype='<U259')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-jXtPpy4zhy",
        "colab_type": "text"
      },
      "source": [
        "Thus we see that our assumption was indeed correct and you can tokenize sentences\n",
        "belonging to different languages in two different ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE8qv_FN40I6",
        "colab_type": "text"
      },
      "source": [
        "#### PunktSentenceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_r9XQtY4sY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "52bbe98e-f4f7-4257-cc01-2c9d55ebc05d"
      },
      "source": [
        "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
        "sample_sentences = punkt_st.tokenize(sample_text)\n",
        "np.array(sample_sentences)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"US unveils world's most powerful supercomputer, beats China.\",\n",
              "       \"The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.\",\n",
              "       'With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.',\n",
              "       'Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.'],\n",
              "      dtype='<U178')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxR3E3QVJlRe",
        "colab_type": "text"
      },
      "source": [
        "#### RegexpTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaDWysh-JmYB",
        "colab_type": "text"
      },
      "source": [
        "The last tokenizer we cover in sentence tokenization is using an instance of the\n",
        "RegexpTokenizer class to tokenize text into sentences, where we will use specific regular expression-based patterns to segment sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yviXAQGsJaxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "4c3b3996-9702-4ae9-9c54-d48c68410d98"
      },
      "source": [
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "np.array(sample_sentences)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"US unveils world's most powerful supercomputer, beats China.\",\n",
              "       \" The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.\",\n",
              "       'With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.',\n",
              "       'Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.'],\n",
              "      dtype='<U178')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg_NoyvJL99s",
        "colab_type": "text"
      },
      "source": [
        "This output shows that we obtained the same sentences as we had obtained using\n",
        "the other tokenizers. This gives us an idea of tokenizing text into sentences using\n",
        "different NLTK interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WQe33J1L-k4",
        "colab_type": "text"
      },
      "source": [
        "### Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQJ6gjvhMBer",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRy302ynLggD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}