{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-preprocessing-and-wrangling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/text-analytics-with-python/blob/3-processing-and-understanding-text/text_preprocessing_and_wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsDEe5jehUQx",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing and Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK8QcV53hVML",
        "colab_type": "text"
      },
      "source": [
        "Text wrangling (also called preprocessing or normalization) is a process that consists of a series of steps to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and intelligent systems powered by machine learning and deep learning. \n",
        "\n",
        "Common techniques for preprocessing include-\n",
        "* cleaning text, \n",
        "* tokenizing text,\n",
        "* removing special characters, \n",
        "* case conversion, \n",
        "* correcting spellings, \n",
        "* removing stopwords\n",
        "* and other unnecessary terms, stemming, and lemmatization.\n",
        "\n",
        "The key idea is to remove unnecessary content from one or more text documents in a corpus (or corpora) and get clean text documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJnaD-5civaR",
        "colab_type": "text"
      },
      "source": [
        "## Removing HTML Tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2im0tkhciw4W",
        "colab_type": "text"
      },
      "source": [
        "Often, unstructured text contains a lot of noise, especially if you use techniques\n",
        "like web scraping or screen scraping to retrieve data from web pages, blogs, and\n",
        "online repositories. HTML tags, JavaScript, and Iframe tags typically don’t add much\n",
        "value to understanding and analyzing text. Our main intent is to extract meaningful\n",
        "textual content from the data extracted from the web.\n",
        "\n",
        "Let’s look at a section of a web page showing the King James version of the Bible.\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/text-analytics-with-python/bible.PNG?raw=1' width='800'/>\n",
        "\n",
        "We will now leverage requests and retrieve the contents of this web page in Python.\n",
        "This is known as web scraping and the following code helps us achieve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_2A3ZMi9iN",
        "colab_type": "code",
        "outputId": "2c057e57-b460-4410-e05e-cab92499fdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.content\n",
        "content[1163:2200]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'content=\"Ebookmaker 0.4.0a5 by Marcello Perathoner &lt;webmaster@gutenberg.org&gt;\" name=\"generator\"/>\\r\\n</head>\\r\\n  <body><p id=\"id00000\">Project Gutenberg EBook The Bible, King James, Book 1: Genesis</p>\\r\\n\\r\\n<p id=\"id00001\">Copyright laws are changing all over the world. Be sure to check the\\r\\ncopyright laws for your country before downloading or redistributing\\r\\nthis or any other Project Gutenberg eBook.</p>\\r\\n\\r\\n<p id=\"id00002\">This header should be the first thing seen when viewing this Project\\r\\nGutenberg file.  Please do not remove it.  Do not change or edit the\\r\\nheader without written permission.</p>\\r\\n\\r\\n<p id=\"id00003\">Please read the \"legal small print,\" and other information about the\\r\\neBook and Project Gutenberg at the bottom of this file.  Included is\\r\\nimportant information about your specific rights and restrictions in\\r\\nhow the file may be used.  You can also find out about how to make a\\r\\ndonation to Project Gutenberg, and how to get involved.</p>\\r\\n\\r\\n<p id=\"id00004\" style=\"margin-top: 2em\">**Welcome To The World of F'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkyy2hVZlmTA",
        "colab_type": "text"
      },
      "source": [
        "We can clearly see from the preceding output that it is extremely difficult to decipher the actual textual content in the web page, due to all the unnecessary HTML tags. We need to remove those tags. \n",
        "\n",
        "The BeautifulSoup library provides us with some handy\n",
        "functions that help us remove these unnecessary tags with ease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OarkpRtlTUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or7kU1a0lxlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "  # remove iframe and script tag\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "\n",
        "  return stripped_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLq5eY-9nExT",
        "colab_type": "code",
        "outputId": "3377c134-2542-4a18-84b4-46d6ad5f3c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "clean_content = strip_html_tags(content)\n",
        "clean_content[1163:2045]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\\nThis eBook was produced by David Widger\\nwith the help of Derek Andrew's text from January 1992\\nand the work of Bryan Taylor in November 2002.\\nBook 01        Genesis\\n01:001:001 In the beginning God created the heaven and the earth.\\n01:001:002 And the earth was without form, and void; and darkness was\\n           upon the face of the deep. And the Spirit of God moved upon\\n           the face of the waters.\\n01:001:003 And God said, Let there be light: and there was light.\\n01:001:004 And God saw the light, that it was good: and God divided the\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0light from the darkness.\\n01:001:005 And God called the light Day, and the darkness he called\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Night. And the evening and the morning were the first day.\\n01:001:006 And God said, Let there be a firmament in the midst of the\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0waters,\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027Hf_S3n6n5",
        "colab_type": "text"
      },
      "source": [
        "You can compare this output with the raw web page content and see that we have\n",
        "successfully removed the unnecessary HTML tags. We now have a clean body of text\n",
        "that’s easier to interpret and understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Ocjr86oOWj",
        "colab_type": "text"
      },
      "source": [
        "## Text Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcnrPoKeoPGH",
        "colab_type": "text"
      },
      "source": [
        "The most popular tokenization techniques include sentence and word tokenization, which are used to break down a text document (or corpus) into sentences and each sentence into words. Thus, tokenization can be defined as the process of breaking down or splitting textual data into smaller and more meaningful components called tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfm-7Ps6pA43",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU2oiOaKpB7C",
        "colab_type": "text"
      },
      "source": [
        "Sentence tokenization is the process of splitting a text corpus into sentences that act as the first level of tokens the corpus is comprised of. This is also known as sentence segmentation, since we try to segment the text into meaningful sentences.\n",
        "\n",
        "There are various ways to perform sentence tokenization. Basic techniques include:-\n",
        "* looking for specific delimiters between sentences like a period (.) \n",
        "* or a newline character (\\n) \n",
        "* and sometimes even a semicolon (;). \n",
        "\n",
        "We will use the NLTK framework, which provides various interfaces for performing sentence tokenization. We primarily focus on the\n",
        "following sentence tokenizers:\n",
        "* sent_tokenize\n",
        "* Pretrained sentence tokenization models\n",
        "* PunktSentenceTokenizer\n",
        "* RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9fJtf9pnNIi",
        "colab_type": "code",
        "outputId": "a52ea0e6-059a-4b69-bc59-16960540bd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjOZQjkJqJNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading text corpora\n",
        "alice = gutenberg.raw(fileids='carroll-alice.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA0x15iTqbCU",
        "colab_type": "code",
        "outputId": "5a2b8242-9a1e-443a-8d82-ad50c29c8e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "sample_text = '''US unveils world's most powerful supercomputer, beats China. \\\n",
        " The US has unveiled the world's most powerful supercomputer called 'Summit', \\\n",
        " beating the previous record-holder China's Sunway TaihuLight. With a peak performance \\\n",
        " of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \\\n",
        " which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \\\n",
        " which reportedly take up the size of two tennis courts.'''\n",
        "sample_text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China.  The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight. With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sEwRqy1rMwv",
        "colab_type": "code",
        "outputId": "9cb8954c-b1c1-414e-d8a6-f5d03d1d3b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Total characters in Alice in Wonderland\n",
        "len(alice)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144395"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg4nesUSsD1h",
        "colab_type": "code",
        "outputId": "8ab136e2-8576-4bf6-c244-d2f5bf1d3f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# First 100 characters in the corpus\n",
        "alice[:100]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNFeUIHUsNBQ",
        "colab_type": "text"
      },
      "source": [
        "#### Default Sentence Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNXcGcc8sOSc",
        "colab_type": "text"
      },
      "source": [
        "The nltk.sent_tokenize(...) function is the default sentence tokenization function\n",
        "that NLTK recommends and it uses an instance of the PunktSentenceTokenizer class\n",
        "internally. However, this is not just a normal object or instance of that class. It has been\n",
        "pretrained on several language models and works really well on many popular languages\n",
        "besides English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_18s3mQOs0ax",
        "colab_type": "code",
        "outputId": "c5c4cc65-2df4-4550-c63b-804a76c77bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFSwQubisIlO",
        "colab_type": "code",
        "outputId": "2a0e63f9-c206-483f-9196-bbd5aca11bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "default_st = nltk.sent_tokenize\n",
        "alice_sentences = default_st(text=alice)\n",
        "sample_sentences = default_st(text=sample_text)\n",
        "print(f'Total sentences in sample_text: {str(len(sample_sentences))}')\n",
        "print(f'Sample text sentences :-\\n{str(np.array(sample_sentences))}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences in sample_text: 4\n",
            "Sample text sentences :-\n",
            "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
            " \"The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.\"\n",
            " 'With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.'\n",
            " 'Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEL4tILvswmW",
        "colab_type": "code",
        "outputId": "ffa66499-0a87-4b59-c34f-ee2dbb6261ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print(f'\\nTotal sentences in alice: {str(len(alice_sentences))}')\n",
        "print(f'First 5 sentences in alice:- \\n {str(np.array(alice_sentences[:5]))}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total sentences in alice: 1625\n",
            "First 5 sentences in alice:- \n",
            " [\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
            " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
            " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
            " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
            " 'Oh dear!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzLoVASlt_oR",
        "colab_type": "text"
      },
      "source": [
        "Now, as you can see, the tokenizer is quite intelligent. It doesn’t just use periods to delimit sentences, but also considers other punctuation and capitalization of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5TG_HmVuFqj",
        "colab_type": "text"
      },
      "source": [
        "#### Pretrained Sentence Tokenizer Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WO0Rza1uHH-",
        "colab_type": "text"
      },
      "source": [
        "Suppose we were dealing with German text. We can use sent_tokenize, which\n",
        "is already trained, or load a pretrained tokenization model on German text into a PunktSentenceTokenizer instance and perform the same operation. The following\n",
        "snippet shows this. We start by loading a German text corpus and inspecting it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNMwlvXq2ERd",
        "colab_type": "code",
        "outputId": "d63cf604-8e37-429b-e785-1cdabf13ae5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "nltk.download('europarl_raw')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package europarl_raw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eB8CmTtlXk",
        "colab_type": "code",
        "outputId": "ee745fcb-4022-4774-84d5-46d8924a95cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk.corpus import europarl_raw\n",
        "\n",
        "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
        "# Total characters in the corpus\n",
        "print(len(german_text))\n",
        "# First 100 characters in the corpus\n",
        "german_text[:100]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBejCRTt3ZuW",
        "colab_type": "text"
      },
      "source": [
        "Next, we tokenize the text corpus into sentences using the default sent_\n",
        "tokenize(...) tokenizer and a pretrained German language tokenizer by loading it\n",
        "from the NLTK resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gURIW3m2Bj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default sentence tokenizer\n",
        "german_sentences_def = default_st(text=german_text, language='german')\n",
        "\n",
        "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
        "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
        "german_sentences = german_tokenizer.tokenize(german_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfJ4siJp4I7Z",
        "colab_type": "text"
      },
      "source": [
        "We can now verify the time of our German tokenizer and check if the results\n",
        "obtained by using the two tokenizers match!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkFouiKI3_sT",
        "colab_type": "code",
        "outputId": "fb57c917-10ad-466d-87be-15257bb1ebc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# verify the type of german_tokenizer, should be PunktSentenceTokenizer\n",
        "type(german_tokenizer)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.tokenize.punkt.PunktSentenceTokenizer"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEMPwJpr4QHL",
        "colab_type": "code",
        "outputId": "7798e3b1-b872-4fa5-b985-cc520e21f43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# check if results of both tokenizers match , should be True\n",
        "(german_sentences_def == german_sentences)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJM8yTVA4mSL",
        "colab_type": "text"
      },
      "source": [
        "Thus we see that indeed the german_tokenizer is an instance of\n",
        "PunktSentenceTokenizer, which specializes in dealing with the German language. We\n",
        "also checked if the sentences obtained from the default tokenizer are the same as the\n",
        "sentences obtained by this pretrained tokenizer. As expected, they are the same (true)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK3fdDJI4bVx",
        "colab_type": "code",
        "outputId": "bb07e9e8-ebff-45a8-822f-da63e333884c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# print first 5 sentences of the corpus\n",
        "np.array(german_sentences[:5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
              "       'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
              "       'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
              "       'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
              "       'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .'],\n",
              "      dtype='<U259')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-jXtPpy4zhy",
        "colab_type": "text"
      },
      "source": [
        "Thus we see that our assumption was indeed correct and you can tokenize sentences\n",
        "belonging to different languages in two different ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE8qv_FN40I6",
        "colab_type": "text"
      },
      "source": [
        "#### PunktSentenceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_r9XQtY4sY1",
        "colab_type": "code",
        "outputId": "c9375586-9e3a-4360-8c54-eb1e9ebb3245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
        "sample_sentences = punkt_st.tokenize(sample_text)\n",
        "np.array(sample_sentences)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"US unveils world's most powerful supercomputer, beats China.\",\n",
              "       \"The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.\",\n",
              "       'With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.',\n",
              "       'Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.'],\n",
              "      dtype='<U178')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxR3E3QVJlRe",
        "colab_type": "text"
      },
      "source": [
        "#### RegexpTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaDWysh-JmYB",
        "colab_type": "text"
      },
      "source": [
        "The last tokenizer we cover in sentence tokenization is using an instance of the\n",
        "RegexpTokenizer class to tokenize text into sentences, where we will use specific regular expression-based patterns to segment sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yviXAQGsJaxW",
        "colab_type": "code",
        "outputId": "7473aba8-c736-4ae0-e015-5dea0bcb85f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
        "sample_sentences = regex_st.tokenize(sample_text)\n",
        "np.array(sample_sentences)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"US unveils world's most powerful supercomputer, beats China.\",\n",
              "       \" The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.\",\n",
              "       'With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.',\n",
              "       'Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.'],\n",
              "      dtype='<U178')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg_NoyvJL99s",
        "colab_type": "text"
      },
      "source": [
        "This output shows that we obtained the same sentences as we had obtained using\n",
        "the other tokenizers. This gives us an idea of tokenizing text into sentences using\n",
        "different NLTK interfaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WQe33J1L-k4",
        "colab_type": "text"
      },
      "source": [
        "### Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQJ6gjvhMBer",
        "colab_type": "text"
      },
      "source": [
        "Word tokenization is the process of splitting or segmenting sentences into their\n",
        "constituent words. A sentence is a collection of words and with tokenization we\n",
        "essentially split a sentence into a list of words that can be used to reconstruct the\n",
        "sentence. Word tokenization is really important in many processes, especially in\n",
        "cleaning and normalizing text where operations like stemming and lemmatization work\n",
        "on each individual word based on its respective stems and lemma. Similar to sentence\n",
        "tokenization, NLTK provides various useful interfaces for word tokenization.\n",
        "\n",
        "* word_tokenize\n",
        "* TreebankWordTokenizer\n",
        "* TokTokTokenizer\n",
        "* RegexpTokenizer\n",
        "* Inherited tokenizers from RegexpTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztiRSr6NLiU",
        "colab_type": "text"
      },
      "source": [
        "#### Default Word Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oqR3brSNM_r",
        "colab_type": "text"
      },
      "source": [
        "The nltk.word_tokenize(...) function is the default and recommended word\n",
        "tokenizer, as specified by NLTK. This tokenizer is an instance or object of the\n",
        "TreebankWordTokenizer class in its internal implementation and acts as a wrapper to that core class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRy302ynLggD",
        "colab_type": "code",
        "outputId": "e9ad26e8-2520-4796-8e7b-8aca2b499907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "default_wt = nltk.word_tokenize\n",
        "words = default_wt(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vg4nf34NryK",
        "colab_type": "text"
      },
      "source": [
        "#### TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN_Ls_XCNtFf",
        "colab_type": "text"
      },
      "source": [
        "The TreebankWordTokenizer is based on the Penn Treebank and uses various regular\n",
        "expressions to tokenize the text. Of course, one primary assumption here is that we have already performed sentence tokenization beforehand.Some of the main features of this tokenizer are mentioned here:\n",
        "\n",
        "* Splits and separates out periods that appear at the end of a sentence\n",
        "* Splits and separates commas and single quotes when followed by\n",
        "whitespace\n",
        "* Most punctuation characters are split and separated into\n",
        "independent tokens\n",
        "* Splits words with standard contractions, such as don’t to do and n’t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sGOTOaHNkUD",
        "colab_type": "code",
        "outputId": "7fb9142f-ef86-4291-e603-9f4ccf7db0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "treebank_wt = nltk.TreebankWordTokenizer()\n",
        "words = treebank_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tAFdoCvOlun",
        "colab_type": "text"
      },
      "source": [
        "As expected, the output is similar to word_tokenize(), since they use the same\n",
        "tokenizing mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yielzRhxOmTB",
        "colab_type": "text"
      },
      "source": [
        "#### TokTokTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw7o196wOqHb",
        "colab_type": "text"
      },
      "source": [
        "TokTokTokenizer is one of the newer tokenizers introduced by NLTK present in the\n",
        "nltk.tokenize.toktok module. In general, the tok-tok tokenizer is a general tokenizer,\n",
        "where it assumes that the input has one sentence per line. Hence, only the final period\n",
        "is tokenized. However, as needed, we can remove the other periods from the words\n",
        "using regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkL2uf3ROZOQ",
        "colab_type": "code",
        "outputId": "04cf5bfa-c567-4012-c813-8e62cc3dd6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "tokenizer = ToktokTokenizer()\n",
        "words = tokenizer.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ravfNHczPPyu",
        "colab_type": "text"
      },
      "source": [
        "#### RegexpTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4czJ1pevPQxf",
        "colab_type": "text"
      },
      "source": [
        "There are two main parameters that\n",
        "are useful in tokenization—the regex pattern for building the tokenizer and the gaps\n",
        "parameter, which, if set to true, is used to find the gaps between the tokens. Otherwise, it\n",
        "is used to find the tokens themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCWyT2kWPDgy",
        "colab_type": "code",
        "outputId": "a1d983ff-63ec-45c2-d39d-127210361ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# pattern to identify tokens themselves\n",
        "TOKEN_PATTERN = r'\\w+'\n",
        "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
        "\n",
        "words = regex_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', 's', 'most', 'powerful', 'supercomputer',\n",
              "       'beats', 'China', 'The', 'US', 'has', 'unveiled', 'the', 'world',\n",
              "       's', 'most', 'powerful', 'supercomputer', 'called', 'Summit',\n",
              "       'beating', 'the', 'previous', 'record', 'holder', 'China', 's',\n",
              "       'Sunway', 'TaihuLight', 'With', 'a', 'peak', 'performance', 'of',\n",
              "       '200', '000', 'trillion', 'calculations', 'per', 'second', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       'which', 'is', 'capable', 'of', '93', '000', 'trillion',\n",
              "       'calculations', 'per', 'second', 'Summit', 'has', '4', '608',\n",
              "       'servers', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaaEmp2zP4ZE",
        "colab_type": "code",
        "outputId": "ced9922e-07a0-4e72-a900-4243bfa5ca8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# pattern to identify tokens by using gaps between tokens\n",
        "GAP_PATTERN = r'\\s+'\n",
        "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
        "\n",
        "words = regex_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
              "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
              "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
              "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
              "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
              "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
              "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
              "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6RuT1ylQtrS",
        "colab_type": "text"
      },
      "source": [
        "Thus, you can see that there are multiple ways of obtaining the same results\n",
        "leveraging token patterns themselves or gap patterns.Lets see how to obtain the token boundaries for each token during the tokenize operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPyIu0pAQYSc",
        "colab_type": "code",
        "outputId": "ae526a26-a0fc-4da5-84d4-35a140cd9a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "word_indices = list(regex_wt.span_tokenize(sample_text))\n",
        "print(word_indices)\n",
        "print(np.array([sample_text[start:end] for start, end in word_indices]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 2), (3, 10), (11, 18), (19, 23), (24, 32), (33, 47), (48, 53), (54, 60), (62, 65), (66, 68), (69, 72), (73, 81), (82, 85), (86, 93), (94, 98), (99, 107), (108, 121), (122, 128), (129, 138), (140, 147), (148, 151), (152, 160), (161, 174), (175, 182), (183, 189), (190, 201), (202, 206), (207, 208), (209, 213), (214, 225), (227, 229), (230, 237), (238, 246), (247, 259), (260, 263), (264, 271), (272, 274), (275, 277), (278, 282), (283, 288), (289, 291), (292, 296), (297, 299), (300, 306), (307, 318), (320, 325), (326, 328), (329, 336), (337, 339), (340, 346), (347, 355), (356, 368), (369, 372), (373, 380), (381, 387), (388, 391), (392, 397), (398, 406), (408, 413), (414, 424), (425, 429), (430, 432), (433, 436), (437, 441), (442, 444), (445, 448), (449, 455), (456, 463)]\n",
            "['US' 'unveils' \"world's\" 'most' 'powerful' 'supercomputer,' 'beats'\n",
            " 'China.' 'The' 'US' 'has' 'unveiled' 'the' \"world's\" 'most' 'powerful'\n",
            " 'supercomputer' 'called' \"'Summit',\" 'beating' 'the' 'previous'\n",
            " 'record-holder' \"China's\" 'Sunway' 'TaihuLight.' 'With' 'a' 'peak'\n",
            " 'performance' 'of' '200,000' 'trillion' 'calculations' 'per' 'second,'\n",
            " 'it' 'is' 'over' 'twice' 'as' 'fast' 'as' 'Sunway' 'TaihuLight,' 'which'\n",
            " 'is' 'capable' 'of' '93,000' 'trillion' 'calculations' 'per' 'second.'\n",
            " 'Summit' 'has' '4,608' 'servers,' 'which' 'reportedly' 'take' 'up' 'the'\n",
            " 'size' 'of' 'two' 'tennis' 'courts.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDEmdq7lRSOP",
        "colab_type": "text"
      },
      "source": [
        "#### Inherited Tokenizers from RegexpTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fix1_uNRTj0",
        "colab_type": "text"
      },
      "source": [
        "Besides the base RegexpTokenizer class, there are several derived classes that\n",
        "perform different types of word tokenization. The WordPunktTokenizer uses the pattern\n",
        "r'\\w+|[^\\w\\s]+' to tokenize sentences into independent alphabetic and\n",
        "non-alphabetic tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCkLh1fKRKR7",
        "colab_type": "code",
        "outputId": "90248ca4-b0cc-4881-ac65-47ca6df5f53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
        "words = wordpunkt_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"',\", 'beating', 'the',\n",
              "       'previous', 'record', '-', 'holder', 'China', \"'\", 's', 'Sunway',\n",
              "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200',\n",
              "       ',', '000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
              "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
              "       ',', 'which', 'is', 'capable', 'of', '93', ',', '000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4', ',',\n",
              "       '608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny2vNh91R2BX",
        "colab_type": "text"
      },
      "source": [
        "The WhitespaceTokenizer tokenizes sentences into words based on whitespace, like\n",
        "tabs, newlines, and spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOiIr-KjRnuE",
        "colab_type": "code",
        "outputId": "2b09e516-c664-48d3-ec1b-649f8f7af4fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "whitespace_wt = nltk.WhitespaceTokenizer()\n",
        "words = whitespace_wt.tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
              "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
              "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
              "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
              "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
              "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
              "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
              "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
              "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
              "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8jBzLqrSInk",
        "colab_type": "text"
      },
      "source": [
        "### Building Robust Tokenizers with NLTK and spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe4qKeKSSLIx",
        "colab_type": "text"
      },
      "source": [
        "For a typical NLP pipeline, I recommend leveraging state-of-the-art libraries like NLTK\n",
        "and spaCy and using some of their robust utilities to build a custom function to perform\n",
        "both sentence- and word-level tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gt8ser_R_hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "  return word_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoWp6vo3UNDk",
        "colab_type": "code",
        "outputId": "e1eb91ca-7764-4a65-d7ca-40f946a72e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "sents = tokenize_text(sample_text)\n",
        "np.array(sents)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQNflRClUaCC",
        "colab_type": "text"
      },
      "source": [
        "We can also get to the level of word-level tokenization by leveraging list\n",
        "comprehensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxCOdPkLUTJe",
        "colab_type": "code",
        "outputId": "7849fa6b-349b-47a3-e75f-440f2bdc056d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "words = [word for sentence in sents for word in sentence]\n",
        "np.array(words)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfu8NpPyU4hD",
        "colab_type": "text"
      },
      "source": [
        "In a similar way, we can leverage spaCy to perform sentence- and word-level\n",
        "tokenizations really quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXS7fy37Vw9B",
        "colab_type": "code",
        "outputId": "a74bb417-7278-489e-eff3-156b1e350db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Initially I download two en packages\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# establish link to packages\n",
        "!python -m spacy download en"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 3.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=68d2def0a7cee8a1d6872ff5c9bab4bc8195db9242be27cb1b5c0e2e0f0eb42e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ea46ttj/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ51YvNvUrEY",
        "colab_type": "code",
        "outputId": "89dae233-f8b8-4106-bb2e-2bd6cda2ac15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
        "text_spacy = nlp(sample_text)\n",
        "\n",
        "sents = np.array(list(text_spacy.sents))\n",
        "sents"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([US unveils world's most powerful supercomputer, beats China.  ,\n",
              "       The US has unveiled the world's most powerful supercomputer called 'Summit',  beating the previous record-holder China's Sunway TaihuLight.,\n",
              "       With a peak performance  of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight,  which is capable of 93,000 trillion calculations per second.,\n",
              "       Summit has 4,608 servers,  which reportedly take up the size of two tennis courts.],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1E0j1K8VST9",
        "colab_type": "code",
        "outputId": "20a0310b-181b-4d13-d369-7f3b090ec988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "sent_words = [[word.text for word in sent] for sent in sents]\n",
        "np.array(sent_words)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', ' ']),\n",
              "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', ' ', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
              "       list(['With', 'a', 'peak', 'performance', ' ', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', ' ', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
              "       list(['Summit', 'has', '4,608', 'servers', ',', ' ', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O4x68eiheQc",
        "colab_type": "code",
        "outputId": "ae673a8f-e49a-412f-dbb2-0fd039d56f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "words = [word.text for word in text_spacy]\n",
        "np.array(words)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', ' ', 'The', 'US',\n",
              "       'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', ' ', 'beating',\n",
              "       'the', 'previous', 'record', '-', 'holder', 'China', \"'s\",\n",
              "       'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance',\n",
              "       ' ', 'of', '200,000', 'trillion', 'calculations', 'per', 'second',\n",
              "       ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway',\n",
              "       'TaihuLight', ',', ' ', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has',\n",
              "       '4,608', 'servers', ',', ' ', 'which', 'reportedly', 'take', 'up',\n",
              "       'the', 'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STv1n0D7hvzK",
        "colab_type": "text"
      },
      "source": [
        "## Removing Accented Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvX1UrmZhxBA",
        "colab_type": "text"
      },
      "source": [
        "Usually in any text corpus, you might be dealing with accented characters/letters, especially\n",
        "if you only want to analyze the English language. Hence, we need to make sure that these\n",
        "characters are converted and standardized into ASCII characters. This shows a simple\n",
        "example — converting é to e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KeKIrk8hpHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JclqjhouiHQ4",
        "colab_type": "code",
        "outputId": "a42bd418-e59d-43f2-d908-b337a6ee1ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hanzzPqXipaG",
        "colab_type": "text"
      },
      "source": [
        "## Expanding Contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks44cYg5iqWe",
        "colab_type": "text"
      },
      "source": [
        "Contractions are shortened versions of words or syllables. These exist in written and\n",
        "spoken forms. Shortened versions of existing words are created by removing specific\n",
        "letters and sounds. In the case of English contractions, they are often created by\n",
        "removing one of the vowels from the word. Examples include “is not” to “isn’t” and\n",
        "“will not” to “won’t”, where you can notice the apostrophe being used to denote the\n",
        "contraction and some of the vowels and other letters being removed.\n",
        "\n",
        "Ideally, you can have a proper mapping for contractions and their corresponding\n",
        "expansions and then use that to expand all the contractions in your text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2U8AK5Pigxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTRACTION_MAP = {\n",
        "  \"ain't\": \"is not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"can not\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"i'd\": \"i would\",\n",
        "  \"i'd've\": \"i would have\",\n",
        "  \"i'll\": \"i will\",\n",
        "  \"i'll've\": \"i will have\",\n",
        "  \"i'm\": \"i am\",\n",
        "  \"i've\": \"i have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it would\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so as\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there would\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we would\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you would\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you will\",\n",
        "  \"you'll've\": \"you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNz9v4BJMjq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "  contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE | re.DOTALL)\n",
        "  def expand_match(contraction):\n",
        "    match = contraction.group(0)\n",
        "    first_char = match[0]\n",
        "    expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
        "    expanded_contraction = first_char + expanded_contraction[1:]\n",
        "\n",
        "    return expanded_contraction\n",
        "\n",
        "  expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "  expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "\n",
        "  return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYEHPxh_O4Am",
        "colab_type": "text"
      },
      "source": [
        "we use the expanded_match function inside the main\n",
        "expand_contractions function to find each contraction that matches the regex pattern\n",
        "we create out of all the contractions in our CONTRACTION_MAP dictionary. On matching\n",
        "any contraction, we substitute it with its corresponding expanded version and retain the\n",
        "correct case of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNybkc9POcSq",
        "colab_type": "code",
        "outputId": "f70119b8-29ac-4e3f-e9fb-4ea1f36f6db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "expand_contractions(\"Y'all can't expand contractions I'd think\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You all can not expand contractions I would think'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAhqqGLGPYh_",
        "colab_type": "text"
      },
      "source": [
        "## Removing Special Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k6QVIdXPaDV",
        "colab_type": "text"
      },
      "source": [
        "Special characters and symbols are usually non-alphanumeric characters or even\n",
        "occasionally numeric characters (depending on the problem), which add to the extra\n",
        "noise in unstructured text. Usually, simple regular expressions (regexes) can be used to\n",
        "remove them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uZ4J3m_O-u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BC4d6PPQCzv",
        "colab_type": "code",
        "outputId": "e2f5b09a-fde4-499b-8b5d-464febe8dd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "remove_special_characters('Well this was fun! What do you think? 123#@!', remove_digits=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR2WuxssQJy_",
        "colab_type": "code",
        "outputId": "8658c71b-2034-46be-f9f1-4c90e60bda1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "remove_special_characters('Well this was fun! What do you think? 123#@!', remove_digits=False)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think 123'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZBM3eOIRGiI",
        "colab_type": "text"
      },
      "source": [
        "## Text Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHuPT3BvRIBL",
        "colab_type": "text"
      },
      "source": [
        "One of the main challenges faced in text wrangling is the presence of incorrect words\n",
        "in the text. The definition of incorrect here covers words that have spelling mistakes as\n",
        "well as words with several letters repeated that do not contribute much to its overall\n",
        "significance.\n",
        "\n",
        "The\n",
        "main objective here is to standardize different forms of these words to the correct form\n",
        "so that we do not end up losing vital information from different tokens in the text. We\n",
        "cover dealing with repeated characters as well as correcting spellings in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRA4VSghIoZ",
        "colab_type": "text"
      },
      "source": [
        "### Correcting Repeating Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G2Vtf8thJ5v",
        "colab_type": "text"
      },
      "source": [
        "We just mentioned words that often contain several repeating characters that could be\n",
        "due to incorrect spellings, slang language, or even people wanting to express strong\n",
        "emotions.\n",
        "\n",
        "The first step in our algorithm is to identify repeated characters in a word using\n",
        "a regex pattern and then use a substitution to remove the characters one by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEztErV_QQNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "209830c5-d8c6-4dad-90b5-60ce88436335"
      },
      "source": [
        "old_word = 'finalllyyy'\n",
        "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "match_substitution = r'\\1\\2\\3'\n",
        "step =1\n",
        "\n",
        "while True:\n",
        "  # remove one repeated character\n",
        "  new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "\n",
        "  if new_word != old_word:\n",
        "    print(f'Step: {str(step)} Word: {str(new_word)}')\n",
        "    step += 1\n",
        "    # update old word to last substituted state\n",
        "    old_word = new_word\n",
        "    continue\n",
        "  else:\n",
        "    print(f'Final word: {new_word}')\n",
        "    break"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 1 Word: finalllyy\n",
            "Step: 2 Word: finallly\n",
            "Step: 3 Word: finally\n",
            "Step: 4 Word: finaly\n",
            "Final word: finaly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcpxRnGtp_rw",
        "colab_type": "text"
      },
      "source": [
        "However, this word is incorrect and the correct\n",
        "word was “finally,” which we had obtained in Step 3. We will now utilize the WordNet\n",
        "corpus to check for valid words at each stage and terminate the loop once it is obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jffoRsJ9rF7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3451ec32-a98c-4367-a754-0e6959477579"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3FDEQsjpw8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "52ee0864-adae-4a83-90c4-042bdb920400"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "old_word = 'finalllyyy'\n",
        "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "match_substitution = r'\\1\\2\\3'\n",
        "step =1\n",
        "\n",
        "while True:\n",
        "  # check for semantically correct word\n",
        "  if wordnet.synsets(old_word):\n",
        "    print(f'Final correct word: {old_word}')\n",
        "    break\n",
        "\n",
        "  # remove one repeated character\n",
        "  new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "  if new_word != old_word:\n",
        "    print(f'Step: {str(step)} Word: {str(new_word)}')\n",
        "    step += 1\n",
        "    # update old word to last substituted state\n",
        "    old_word = new_word\n",
        "    continue\n",
        "  else:\n",
        "    print(f'Final word: {new_word}')\n",
        "    break"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 1 Word: finalllyy\n",
            "Step: 2 Word: finallly\n",
            "Step: 3 Word: finally\n",
            "Final correct word: finally\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOxnYcjrv6N",
        "colab_type": "text"
      },
      "source": [
        "We can build a\n",
        "better version of this code by writing the logic in a function, as depicted here, to make it\n",
        "more generic to deal with incorrect tokens from a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT8mKoqwrBml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_repeated_characters(tokens):\n",
        "  repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "  match_substitution = r'\\1\\2\\3'\n",
        "\n",
        "  def replace(old_word):\n",
        "    if wordnet.synsets(old_word):\n",
        "      return old_word\n",
        "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "    return replace(new_word) if new_word != old_word else new_word\n",
        "\n",
        "  correct_tokens = [replace(word) for word in tokens]\n",
        "  return correct_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1MwKfc2sooE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "305eeb2f-e672-40fe-cfbf-eee8230dbdb6"
      },
      "source": [
        "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
        "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
        "' '.join(correct_tokens)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My school is really amazing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YbpEpKPwPsm",
        "colab_type": "text"
      },
      "source": [
        "We can see from this output that our function performs as intended and replaces the repeating characters in each token, giving us correct tokens as desired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ8EDtB6wbiw",
        "colab_type": "text"
      },
      "source": [
        "### Correcting Spellings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBWoNXeLwcrz",
        "colab_type": "text"
      },
      "source": [
        "The second problem we face with words is incorrect or wrong spellings that occur due to\n",
        "human error and even machine based errors, which you might have seen with features\n",
        "like auto-correcting text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIeZX5V7wJzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}